"""A module for tokenizing a saving a raw text dataset"""

from typing import Iterable

import tensorflow as tf
from transformers import BertTokenizer
from datasets import Dataset as HgfDataset

from ae_sentence_embeddings.modeling_tools import make_decoder_inputs


def tokenize_hgf_dataset(
        dataset: HgfDataset,
        tokenizer: BertTokenizer,
        text_col_names: Iterable[str] = ("text",),
        input_ids_name: str = "input_ids",
        target_prefix: str = "target",
        target_pad: int = -1,
        remove_old_cols: bool = False
) -> HgfDataset:
    """Tokenize a dataset

    Args:
        dataset: The input dataset
        tokenizer: The tokenizer model
        text_col_names: Text column names in the input dataset. Defaults to `("text",)`
        input_ids_name: The name of the column returned by the tokenizer that contains token IDs.
                        Defaults to `input_ids`
        target_prefix: Target column prefix. Target columns contain the token IDs that are to be
                       generated by a decoder. Defaults to `target`
        target_pad: Padding ID for target token IDs. Defaults to `-1`
        remove_old_cols: Set to `True` if the original columns are to be removed. Defaults to `False`

    Returns:
        The tokenized dataset with feature and target columns
    """
    if len(list(text_col_names)) != len(set(text_col_names)):
        raise ValueError("All text column names are required to be unique")
    cols_to_remove = list(dataset.features.keys()) if remove_old_cols else None

    def tok_func(example):
        all_cols = {}
        for text_col_name in text_col_names:
            res = tokenizer(example[text_col_name], return_token_type_ids=False,
                            return_tensors='tf', truncation=True)
            text_col_name_stripped = text_col_name.lstrip("text")
            custom_res = {}
            for feature_col_name, feature_value in res.items():
                if feature_col_name == input_ids_name:
                    target_col_name = ''.join([target_prefix, text_col_name_stripped])
                    custom_res[target_col_name] = tf.squeeze(
                        make_decoder_inputs(feature_value, pad_value=target_pad))
                new_col_name = ''.join([feature_col_name, text_col_name_stripped])
                custom_res[new_col_name] = tf.squeeze(feature_value)
            all_cols.update(custom_res)
        return all_cols

    return dataset.map(tok_func, batched=False, remove_columns=cols_to_remove)
