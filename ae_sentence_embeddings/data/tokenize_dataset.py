# -*- coding: utf-8 -*-

"""A module for tokenizing a raw text dataset for autoregressive modelling
and for classification.
"""

from typing import Dict, Union, List, Sequence, Optional

from datasets import Dataset as HgfDataset, load_dataset
from transformers import PreTrainedTokenizer, PreTrainedTokenizerFast, AutoTokenizer, BatchEncoding


def load_tokenizer(tokenizer_path: str) -> AutoTokenizer:
    """Load a pre-trained tokenizer.

    Args:
        tokenizer_path: Tokenizer name or path. This can be a path to a directory or
            the name of a tokenizer downloadable from the Hugging Face Hub.

    Returns:
         The loaded tokenizer.
    """
    tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)
    return tokenizer


def load_hgf_dataset(dataset_path: str) -> HgfDataset:
    """Load a `datasets.Dataset` in raw text format from disc."""
    # `split = "train" has to be specified, but it has no meaning.`
    return load_dataset("text", data_files=[dataset_path], split="train")


def tokenize_hgf_dataset(
        dataset: HgfDataset,
        tokenizer: Union[PreTrainedTokenizer, PreTrainedTokenizerFast],
        text_col_name: str = "text",
        target_col_name: str = "target",
        target_pad: int = -1,
        remove_old_cols: bool = False,
) -> HgfDataset:
    """Tokenize a dataset.

    Args:
        dataset: The input dataset.
        tokenizer: The tokenizer model.
        text_col_name: Text column names in the input dataset. Defaults to `'text',`.
        target_col_name: Target column name. Target columns contain the token IDs that are to be
            generated by a decoder. Defaults to `'target'`.
        target_pad: Padding ID for target token IDs. Defaults to `-1`.
        remove_old_cols: Set to `True` if the original columns are to be removed. Defaults to `False`.

    Returns:
        The tokenized dataset with feature and target columns.
    """
    cols_to_remove = list(dataset.features.keys()) if remove_old_cols else None

    def tok_func(example: Dict[str, Union[str, int, List]]) -> BatchEncoding:
        res = tokenizer(example[text_col_name], return_token_type_ids=False, truncation=True)
        targets = res["input_ids"][1:]
        targets.append(target_pad)
        res[target_col_name] = targets
        return res

    dataset = dataset.map(tok_func, batched=False, remove_columns=cols_to_remove)
    return dataset


def tokenize_labelled_sequences(
        dataset: HgfDataset,
        tokenizer: Union[PreTrainedTokenizer, PreTrainedTokenizerFast],
        text_col_names: Sequence[str],
        label_col_name: str,
        max_length: Optional[int] = None,
        return_token_type_ids: bool = False,
        remove_old_cols: bool = False
) -> HgfDataset:
    """Tokenized a dataset with labelled text sequences.

    Args:
        dataset: The raw dataset with the texts and the labels.
        tokenizer: A tokenizer model.
        text_col_names: The names of the columns that contain the texts
            (typically sentences) that are to be tokenized.
        label_col_name: The name of the column that contains the labels.
        max_length: Optional. Maximum sequence length above which truncation will be applied.
        return_token_type_ids: If `True`, binary token-type IDs will be created
            by the tokenizer. This is useful only if there are multiple text columns.
            Defaults to `False`.
        remove_old_cols: If `True`, the columns of the input dataset will
            be removed, except the label columns. Defaults to `False`.

    Returns:
        The tokenized dataset. It always contains the columns `input_ids` and `attention_mask`.
    """
    if remove_old_cols:
        cols_to_remove = [col for col in dataset.features.keys() if col != label_col_name]
    else:
        cols_to_remove = None
    if max_length is not None:
        tokenizer.model_max_length = max_length

    def tokenization_func(example: Dict[str, Union[str, int, List]]) -> BatchEncoding:
        text_cols = (example[text_col_name] for text_col_name in text_col_names)
        return tokenizer(*text_cols, truncation=True,
                         return_token_type_ids=return_token_type_ids)

    return dataset.map(tokenization_func, remove_columns=cols_to_remove)
